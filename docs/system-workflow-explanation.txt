PDF SEARCH SYSTEM: COMPLETE WORKFLOW EXPLANATION
==================================================

1. PDF UPLOAD AND PROCESSING WORKFLOW
=====================================

Step 1: File Upload Interface
-----------------------------
uploaded_files = st.file_uploader(
    "Choose PDF files",
    type="pdf", 
    accept_multiple_files=True
)

What happens:
- Streamlit creates a file upload widget
- Accepts multiple PDFs simultaneously
- Files are temporarily stored in browser memory
- Each file object contains: name, size, type, and binary content

Step 2: File Validation
-----------------------
if uploaded_files:
    for uploaded_file in uploaded_files:
        # File size check, type validation
        st.metric("File Size", f"{uploaded_file.size / 1024:.1f} KB")

Validation checks:
- File type must be .pdf
- File size limits (prevents system overload)
- File integrity (not corrupted)
- Duplicate detection preparation

2. TEXT EXTRACTION AND CHUNKING PROCESS
=======================================

Step 1: PDF Text Extraction
---------------------------
def extract_text_from_pdf(pdf_file):
    pdf_reader = PyPDF2.PdfReader(pdf_file)
    text_content = ""
    
    for page_num in range(len(pdf_reader.pages)):
        page = pdf_reader.pages[page_num]
        text_content += page.extract_text() + "\n"
    
    return text_content.strip()

How PyPDF2 works:
1. PDF Structure Analysis: Reads PDF internal structure (objects, streams, fonts)
2. Page Processing: Iterates through each page sequentially
3. Text Object Extraction: Identifies text objects within page content streams
4. Font Mapping: Maps font encodings to Unicode characters
5. Layout Reconstruction: Attempts to preserve reading order and spacing

Extraction challenges handled:
- Scanned PDFs: PyPDF2 can't extract text from images (would need OCR)
- Complex layouts: Multi-column text may be extracted out of order
- Special characters: Non-standard fonts may cause character substitution
- Encrypted PDFs: Password-protected files require credentials

Step 2: Smart Text Chunking
---------------------------
def chunk_text(text, chunk_size=1000, overlap=200):
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        
        # Find natural boundaries
        if end < len(text):
            last_period = chunk.rfind('.')
            last_space = chunk.rfind(' ')
            boundary = max(last_period, last_space)
            
            if boundary > start + chunk_size // 2:
                end = start + boundary + 1
                chunk = text[start:end]
        
        chunks.append({
            "text": chunk.strip(),
            "start_position": start,
            "end_position": end,
            "chunk_id": len(chunks)
        })
        
        start = end - overlap
    
    return chunks

Why chunking is necessary:
- Search Relevance: Smaller chunks provide more precise search results
- Processing Limits: Large documents exceed processing capabilities
- Context Preservation: Maintains meaningful text segments
- Performance: Faster indexing and searching of smaller text blocks

Chunking algorithm details:
1. Size Optimization: 1000 characters = ~150-200 words (optimal for search)
2. Boundary Detection: Finds sentence endings (.) or word boundaries (spaces)
3. Overlap Strategy: 200-character overlap prevents information loss at boundaries
4. Metadata Tracking: Records position information for result highlighting

Example chunking:
Original text: "Machine learning algorithms require large datasets. Deep learning models use neural networks. These systems can..."

Chunk 1: "Machine learning algorithms require large datasets. Deep learning models use neural networks. These..." (chars 0-200)
Chunk 2: "...neural networks. These systems can process complex patterns..." (chars 150-350)

3. OPENSEARCH INDEXING MECHANISM
================================

Step 1: Index Creation and Mapping
----------------------------------
index_mapping = {
    "mappings": {
        "properties": {
            "filename": {"type": "text"},
            "content": {"type": "text"},
            "chunk_id": {"type": "integer"},
            "start_position": {"type": "integer"},
            "end_position": {"type": "integer"},
            "upload_date": {"type": "date"},
            "file_hash": {"type": "keyword"}
        }
    }
}
client.indices.create(index="pdf_documents", body=index_mapping, ignore=400)

OpenSearch index structure:
- Index: Like a database table, stores all PDF documents
- Mapping: Defines data types and how fields should be analyzed
- Sharding: Data distributed across multiple nodes (if clustered)
- Replication: Backup copies for high availability

Field type explanations:
- text: Full-text searchable, analyzed for keywords
- keyword: Exact match only, used for filtering/aggregations
- integer: Numeric values for sorting/range queries
- date: Timestamp fields for temporal queries

Step 2: Document Preprocessing
-----------------------------
# Generate file hash for deduplication
file_hash = hashlib.md5(text_content.encode()).hexdigest()

# Check for existing documents
existing_docs = client.search(
    index="pdf_documents",
    body={"query": {"term": {"file_hash": file_hash}}},
    size=1
)

Deduplication process:
1. Hash Generation: MD5 hash of full text content creates unique fingerprint
2. Collision Check: Search existing documents for matching hash
3. Conflict Resolution: Prevent duplicate indexing of same content
4. Version Control: Track document versions if needed

Step 3: Chunk Indexing
----------------------
for chunk in chunks:
    doc = {
        "filename": pdf_name,
        "content": chunk["text"],
        "chunk_id": chunk["chunk_id"],
        "start_position": chunk["start_position"],
        "end_position": chunk["end_position"],
        "upload_date": datetime.now().isoformat(),
        "file_hash": file_hash
    }
    
    response = client.index(index="pdf_documents", body=doc)

What happens during indexing:

1. Document Analysis: 
   - Text is tokenized into individual words
   - Stop words (the, and, or) are often filtered
   - Terms are converted to lowercase
   - Stemming may reduce words to root forms

2. Inverted Index Creation:
   Term → Document locations
   "machine" → [doc1_chunk3, doc5_chunk1, doc12_chunk7]
   "learning" → [doc1_chunk3, doc1_chunk5, doc5_chunk1]

3. Scoring Preparation:
   - Term frequency (TF): How often term appears in document
   - Inverse document frequency (IDF): How rare term is across collection
   - Field length normalization: Shorter documents may score higher

Step 4: AWS OpenSearch Integration
---------------------------------
auth = AWSV4SignerAuth(credentials, region, service)
client = OpenSearch(
    hosts=[{"host": host, "port": 443}],
    http_auth=auth,
    use_ssl=True,
    verify_certs=True,
    connection_class=RequestsHttpConnection
)

AWS security and performance:
- IAM Authentication: AWS credentials verify access permissions
- Encryption: SSL/TLS encrypts data in transit
- VPC Security: Network isolation if configured
- Auto-scaling: AWS manages cluster scaling based on load
- Backups: Automated snapshots protect against data loss

4. SEARCH QUERY PROCESSING AND RANKING
======================================

Step 1: Query Construction
-------------------------
search_body = {
    "query": {
        "bool": {
            "should": [
                {"match": {"content": {"query": query, "boost": 2}}},
                {"match_phrase": {"content": {"query": query, "boost": 3}}},
                {"match": {"filename": {"query": query, "boost": 1}}}
            ]
        }
    },
    "highlight": {
        "fields": {
            "content": {
                "fragment_size": 200,
                "number_of_fragments": 3
            }
        }
    },
    "sort": [{"_score": {"order": "desc"}}],
    "size": size
}

Query breakdown:

1. Bool Query Structure:
   - should: Acts like OR - any matching clause increases score
   - Multiple clauses combine scores additively
   - More matches = higher relevance score

2. Match Types:
   - match: Analyzes query, finds documents with any matching terms
   - match_phrase: Exact phrase matching with proximity
   - Cross-field search: Searches both content and filename

3. Boost Factors:
   - Content match: 2x boost (main text search)
   - Phrase match: 3x boost (exact phrases rank higher)
   - Filename match: 1x boost (title relevance)

Step 2: Query Analysis and Processing
------------------------------------
User query: "machine learning algorithms"

OpenSearch analysis:
1. Tokenization: ["machine", "learning", "algorithms"]
2. Lowercase: ["machine", "learning", "algorithms"]
3. Stop word removal: [no stop words in this query]
4. Stemming: ["machin", "learn", "algorithm"] (optional)

Query execution path:
1. Query parsing: Break down complex query syntax
2. Term analysis: Apply same analysis as indexing
3. Index lookup: Find documents containing terms
4. Score calculation: Compute relevance scores
5. Result aggregation: Combine and rank results

Step 3: Relevance Scoring Algorithm
-----------------------------------

TF-IDF Scoring Components:

1. Term Frequency (TF):
   TF = sqrt(frequency of term in document)
   - Higher frequency → higher score
   - Square root prevents over-weighting repeated terms

2. Inverse Document Frequency (IDF):
   IDF = log(total documents / documents containing term)
   - Rare terms score higher than common terms
   - "machine" is common → lower IDF
   - "algorithm" is rarer → higher IDF

3. Field Length Normalization:
   norm = 1 / sqrt(field length)
   - Shorter documents may rank higher
   - Prevents long documents from dominating

4. Final Score:
   score = TF × IDF × norm × boost

Example scoring:
Document 1: "Machine learning algorithms are powerful tools..."
- TF(machine): sqrt(2) = 1.41
- IDF(machine): log(1000/500) = 0.69
- Boost: 2.0
- Score component: 1.41 × 0.69 × 2.0 = 1.95

Document 2: "Neural networks and algorithms..."
- TF(algorithm): sqrt(1) = 1.0
- IDF(algorithm): log(1000/200) = 1.61
- Boost: 2.0
- Score component: 1.0 × 1.61 × 2.0 = 3.22

Step 4: Result Highlighting
---------------------------
"highlight": {
    "fields": {
        "content": {
            "fragment_size": 200,
            "number_of_fragments": 3
        }
    }
}

Highlighting process:
1. Fragment Extraction: 200-character snippets around matches
2. HTML Wrapping: Adds <em> tags around matching terms
3. Context Preservation: Shows surrounding text for context
4. Multiple Fragments: Up to 3 best matching snippets per document

Example highlighted result:
"Machine <em>learning</em> <em>algorithms</em> require large datasets to train effectively. Deep <em>learning</em> models use neural networks..."

Step 5: Result Ranking and Sorting
----------------------------------
"sort": [{"_score": {"order": "desc"}}]

Ranking factors:
1. Primary: Relevance score (TF-IDF + boosts)
2. Secondary: Could add date, filename alphabetical, etc.
3. Tie-breaking: Internal document ID for consistent ordering

Result structure:
{
  "hits": {
    "total": {"value": 42},
    "hits": [
      {
        "_score": 8.234,
        "_source": {
          "filename": "AI_Guide.pdf",
          "content": "Machine learning algorithms...",
          "chunk_id": 3
        },
        "highlight": {
          "content": ["Machine <em>learning</em> <em>algorithms</em>..."]
        }
      }
    ]
  }
}

5. DOCUMENT STATISTICS CALCULATION
==================================

Step 1: Index Statistics Retrieval
----------------------------------
index_stats = client.indices.stats(index="pdf_documents")
doc_count = index_stats['indices']['pdf_documents']['total']['docs']['count']
index_size = index_stats['indices']['pdf_documents']['total']['store']['size_in_bytes']

What these stats represent:

1. Document Count:
   - Total number of chunks indexed (not unique PDFs)
   - Each chunk = one document in OpenSearch
   - Example: 5 PDFs × 10 chunks each = 50 total documents

2. Index Size:
   - Physical storage space used in bytes
   - Includes original text + inverted index + metadata
   - Typically 2-3x larger than original text due to indexing overhead

3. Additional available stats:
   {
     "docs": {"count": 1247, "deleted": 0},
     "store": {"size_in_bytes": 52428800},
     "indexing": {"index_total": 1247},
     "search": {"query_total": 89, "query_time_in_millis": 234}
   }

Step 2: Unique Document Counting
--------------------------------
unique_files = client.search(
    index="pdf_documents",
    body={
        "size": 0,  # Don't return actual documents
        "aggs": {
            "unique_files": {
                "cardinality": {
                    "field": "file_hash"
                }
            }
        }
    }
)
unique_count = unique_files['aggregations']['unique_files']['value']

Cardinality aggregation explained:
- Purpose: Count distinct values in a field
- Algorithm: HyperLogLog probabilistic counting
- Accuracy: ~97% accurate, uses less memory than exact counting
- Use case: Count unique PDFs even though we have many chunks per PDF

Why this matters:
Scenario: 
- 3 unique PDF files uploaded
- File 1: 15 chunks
- File 2: 8 chunks  
- File 3: 12 chunks

Results:
- Total chunks: 35
- Unique documents: 3

Step 3: Recent Documents Query
-----------------------------
recent_docs = client.search(
    index="pdf_documents",
    body={
        "size": 10,
        "query": {"match_all": {}},
        "sort": [{"upload_date": {"order": "desc"}}],
        "collapse": {"field": "file_hash"}  # Group by unique document
    }
)

Query breakdown:
1. match_all: Return all documents (no filtering)
2. sort: Order by upload date (newest first)
3. collapse: Group by file_hash to show only one result per unique PDF
4. size: 10: Limit to 10 most recent unique documents

Collapse feature:
- Prevents showing multiple chunks from same PDF
- Shows only the most recent chunk per unique document
- Essential for meaningful "recent documents" display

Step 4: Advanced Statistics (Possible Extensions)
-------------------------------------------------

Document size distribution:
size_stats = client.search(
    index="pdf_documents", 
    body={
        "size": 0,
        "aggs": {
            "doc_sizes": {
                "histogram": {
                    "script": "doc['content'].value.length()",
                    "interval": 500
                }
            }
        }
    }
)

Search analytics:
search_stats = client.search(
    index="pdf_documents",
    body={
        "size": 0,
        "aggs": {
            "upload_timeline": {
                "date_histogram": {
                    "field": "upload_date",
                    "calendar_interval": "day"
                }
            }
        }
    }
)

Content analysis:
term_stats = client.search(
    index="pdf_documents",
    body={
        "size": 0,
        "aggs": {
            "popular_terms": {
                "terms": {
                    "field": "content",
                    "size": 20
                }
            }
        }
    }
)

COMPLETE SYSTEM FLOW DIAGRAM
============================

📄 PDF Upload
    ↓
🔍 File Validation
    ↓
📖 Text Extraction (PyPDF2)
    ↓
✂️  Smart Chunking (1000 chars + overlap)
    ↓
🔐 Hash Generation (MD5)
    ↓
❓ Duplicate Check
    ↓
📚 OpenSearch Indexing
    ↓
🎯 Inverted Index Creation
    ↓
💾 AWS Storage

🔍 Search Query
    ↓
🧠 Query Analysis
    ↓
📊 TF-IDF Scoring
    ↓
🏆 Result Ranking
    ↓
🌟 Highlighting
    ↓
📋 Result Display

📊 Statistics Request
    ↓
📈 Index Stats API
    ↓
🔢 Aggregation Queries
    ↓
📊 Metrics Display

KEY PERFORMANCE CHARACTERISTICS
===============================

Scalability:
- Horizontal: Add more OpenSearch nodes as data grows
- Vertical: Increase memory/CPU for better performance
- Caching: Frequent queries cached for faster response

Accuracy:
- Precision: Relevant results in top positions
- Recall: Finds most relevant documents that exist
- Ranking: Most relevant documents appear first

Speed:
- Indexing: ~1-2 seconds per PDF (depending on size)
- Search: Sub-second response times for most queries
- Statistics: Real-time metrics with minimal delay

Reliability:
- Deduplication: Prevents duplicate content
- Error handling: Graceful failure for problematic PDFs
- Data persistence: AWS ensures data durability

This system creates a powerful, scalable document search engine that transforms static PDFs into a searchable knowledge base with intelligent ranking and comprehensive analytics!